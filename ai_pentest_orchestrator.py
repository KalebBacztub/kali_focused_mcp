# ai_pentest_orchestrator.py

import os
import json
import asyncio
import nest_asyncio
from typing import List, Dict, Any

from dotenv import load_dotenv
from openai import AsyncOpenAI # For OpenRouter
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

nest_asyncio.apply() # Allows asyncio event loop nesting
load_dotenv() # Load .env file for API keys

class AIPentestOrchestrator:
    def __init__(self, openrouter_model_name: str):
        self.mcp_session: ClientSession = None
        self.available_mcp_tools: List[Dict[str, Any]] = []

        self.openrouter_api_key = os.getenv("OPENROUTER_API_KEY")
        if not self.openrouter_api_key:
            raise ValueError("OPENROUTER_API_KEY not found. Please set it in your .env file.")

        self.llm_client = AsyncOpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=self.openrouter_api_key,
        )
        self.model_name = openrouter_model_name
        print(f"[Orchestrator] Initialized with LLM: {self.model_name}")

    async def _format_mcp_tools_for_llm(self, mcp_tools_response) -> List[Dict[str, Any]]:
        """Converts tools from MCP server format to OpenAI LLM format."""
        formatted_tools = []
        if mcp_tools_response and mcp_tools_response.tools:
            for tool in mcp_tools_response.tools:
                input_schema_for_llm = tool.inputSchema if hasattr(tool, 'inputSchema') and tool.inputSchema else {"type": "object", "properties": {}}
                formatted_tools.append({
                    "type": "function",
                    "function": {
                        "name": tool.name,
                        "description": tool.description or f"MCP tool: {tool.name}",
                        "parameters": input_schema_for_llm
                    }
                })
        return formatted_tools

    async def execute_task(self, initial_prompt: str, target_info: Dict[str, Any] = None, max_llm_steps: int = 5):
        """
        Executes a given task by interacting with the LLM and MCP tools.
        """
        if not self.mcp_session:
            print("[Orchestrator] Error: MCP session not established. Cannot execute task.")
            return

        print(f"\nüéØ Starting Task: {initial_prompt}")
        if target_info:
            print(f"   Target Info: {target_info}")

        # Construct a more detailed initial system message or user prompt if needed
        # For example, you can inject target_info into the prompt.
        # This example uses a simple initial user message.
        current_prompt_with_context = initial_prompt
        if target_info and target_info.get("ip"): # Example of adding context
            current_prompt_with_context += f"\nThe primary target IP address is {target_info.get('ip')}."


        messages: List[Dict[str, Any]] = [{'role': 'user', 'content': current_prompt_with_context}]

        for step in range(max_llm_steps):
            print(f"\n--- LLM Step {step + 1}/{max_llm_steps} ---")
            
            try:
                print(f"[Orchestrator] Sending prompt to {self.model_name}...")
                # print(f"DEBUG Messages: {json.dumps(messages, indent=2)}") # For debugging
                # print(f"DEBUG Tools: {json.dumps(self.available_mcp_tools, indent=2)}") # For debugging

                api_response = await self.llm_client.chat.completions.create(
                    model=self.model_name,
                    messages=messages,
                    tools=self.available_mcp_tools if self.available_mcp_tools else None,
                    tool_choice="auto",
                    max_tokens=2048,
                    temperature=0.5 # Adjust as needed
                )
            except Exception as e:
                print(f"[Orchestrator] ‚ùå LLM API Call Error: {e}")
                break # Stop task on API error

            if not api_response or not api_response.choices or not api_response.choices[0]:
                print("[Orchestrator] ‚ùå Error: LLM API returned an empty or invalid response structure.")
                break 
            
            response_message = api_response.choices[0].message
            messages.append(response_message.model_dump(exclude_none=True)) # Add assistant's response to history

            if response_message.tool_calls:
                print(f"[Orchestrator] üõ†Ô∏è LLM requests {len(response_message.tool_calls)} tool call(s):")
                tool_results_for_next_turn = []

                for tool_call in response_message.tool_calls:
                    tool_name = tool_call.function.name
                    tool_call_id = tool_call.id
                    tool_args_str = tool_call.function.arguments
                    
                    print(f"  üìû Attempting to call MCP tool: '{tool_name}' (ID: {tool_call_id})")
                    print(f"     Raw Arguments: {tool_args_str}")

                    try:
                        tool_args = json.loads(tool_args_str)
                        print(f"     Parsed Arguments: {tool_args}")
                        
                        print(f"  ‚è≥ Sending '{tool_name}' call to MCP server...")
                        mcp_tool_response = await self.mcp_session.call_tool(tool_name, arguments=tool_args)
                        tool_result_content = str(mcp_tool_response.content) # Ensure it's a string
                        print(f"  ‚úÖ MCP Tool '{tool_name}' executed. Result snippet: {tool_result_content[:150]}...")
                    except json.JSONDecodeError as e:
                        print(f"  ‚ö†Ô∏è Error decoding JSON arguments for tool '{tool_name}': {e}")
                        tool_result_content = f"Error: Invalid JSON arguments from LLM: {tool_args_str}. Detail: {e}"
                    except Exception as e:
                        print(f"  üí• Error calling MCP tool '{tool_name}': {e}")
                        tool_result_content = f"Error executing tool '{tool_name}' via MCP: {str(e)}"
                    
                    tool_results_for_next_turn.append({
                        "role": "tool",
                        "tool_call_id": tool_call_id,
                        "name": tool_name,
                        "content": tool_result_content,
                    })
                
                for res in tool_results_for_next_turn: # Add all tool results to messages
                    messages.append(res)
                # Loop back for next LLM interaction with tool results

            elif response_message.content:
                print(f"[Orchestrator] üí¨ LLM Response (no tool call):\n{response_message.content}")
                # Here you might check if the LLM's response indicates task completion
                # For now, we'll just print it and assume it might be a final answer or observation.
                # If it's a final answer, we could break the loop.
                print(f"--- Task Step {step + 1} Concluded with LLM text response ---")
                # break # Uncomment if a text response means the task step or task is done
                # For a multi-step task, you might want to continue unless LLM says "I'm done"
                if step == max_llm_steps -1: # If it's the last step and it's text, task is done.
                     print("[Orchestrator] Max LLM steps reached. Concluding task.")
                # If the task requires more steps even after a text response, remove the break.
                # For now, let's assume any text response means the LLM thinks it's done with *this step* or the task.
                # We might need a more sophisticated way to determine task completion.
                # For this example, let's assume a text response might be a concluding remark for the step.
                # if this text response is the end of the task, we should break:
                # For now, let's make it break for simplicity if we get any text response.
                print("[Orchestrator] LLM provided a text response. Assuming this step/task is concluded.")
                break

            else:
                print("[Orchestrator] ‚ö†Ô∏è LLM responded with no text content and no tool calls. Ending task.")
                break
        else: # This else clause executes if the loop completed naturally (didn't break)
             print(f"[Orchestrator] Max LLM steps ({max_llm_steps}) reached. Task ended.")
        
        print(f"üèÅ Task '{initial_prompt}' Finished.")
        # Here you would add logging of the entire 'messages' history for this task for analysis

    async def run_main_scenario(self):
        # --- Configure Server Launch Command (IMPORTANT: Adjust for your setup) ---
        # This needs to correctly start your mcp_basic_tools_server.py
        server_executable = "C:/Users/kbacz/AppData/Local/Microsoft/WindowsApps/python3.11.exe" # YOUR PYTHON PATH
        server_script = "mcp_basic_tools_server.py" # Assumes it's in the same dir or cwd is set
        
        server_params = StdioServerParameters(
            command=server_executable,
            args=[server_script],
            # cwd ensures the server script is found if both orchestrator and server are in the same folder
            cwd=os.path.dirname(os.path.abspath(__file__)),
            env=os.environ.copy(), # Pass current environment
        )
        print(f"[Orchestrator] Attempting to start MCP server: \"{server_executable}\" {server_script}")

        try:
            async with stdio_client(server_params) as (read, write):
                print("[Orchestrator] stdio_client streams established with MCP server process.")
                async with ClientSession(read, write) as session:
                    self.mcp_session = session
                    print("[Orchestrator] MCP ClientSession created. Initializing...")
                    await self.mcp_session.initialize()
                    print("[Orchestrator] MCP Session initialized with server.")

                    mcp_tools_response = await self.mcp_session.list_tools()
                    self.available_mcp_tools = await self._format_mcp_tools_for_llm(mcp_tools_response)
                    
                    if self.available_mcp_tools:
                        print(f"[Orchestrator] ‚úÖ Tools discovered from MCP server: {[tool['function']['name'] for tool in self.available_mcp_tools]}")
                    else:
                        print("[Orchestrator] ‚ö†Ô∏è No tools found on the MCP server. Cannot proceed with tasks requiring tools.")
                        return

                    # --- Define Your Tasks/Scenarios Here ---
                    # For now, one simple task:
                    # Replace with your target IP from your Proxmox vulnerable VM
                    vulnerable_vm_ip = "10.0.0.134" # !!! REPLACE THIS !!!
                    if vulnerable_vm_ip == "192.168.x.y":
                        print("\n‚ö†Ô∏è WARNING: Please replace 'vulnerable_vm_ip' in the script with the actual IP of your target VM!\n")
                        # For safety, let's not proceed if the placeholder IP is still there.
                        # You can comment out the return if you understand and want to proceed with a placeholder.
                        # return 

                    tasks = [
                        {
                            "id": "task_001_ping_scan",
                            "prompt": f"Your first task is reconnaissance. Ping the target host to see if it's online. Then, try to get the homepage of the webserver if it's hosting one.",
                            "target_info": {"ip": vulnerable_vm_ip, "web_port": 80} # Example target info
                        },
                        # Add more tasks for your "set series of framed questions"
                        # {
                        #     "id": "task_002_...",
                        #     "prompt": "...",
                        #     "target_info": {...}
                        # }
                    ]

                    for task in tasks:
                        await self.execute_task(
                            initial_prompt=task["prompt"],
                            target_info=task.get("target_info"),
                            max_llm_steps=7 # Increase steps if needed for multi-tool tasks
                        )
                        print("-" * 50) # Separator between tasks

        except ConnectionRefusedError:
            print(f"[Orchestrator] ‚ùå ConnectionRefusedError: Could not connect to the MCP server. Ensure it started correctly.")
            print(f"   Tried to run: \"{server_executable}\" {server_script}")
        except FileNotFoundError:
            print(f"[Orchestrator] ‚ùå FileNotFoundError: Could not find the server script or the python executable.")
            print(f"   Python Executable: \"{server_executable}\"")
            print(f"   Server Script    : \"{server_script}\" (expected in dir: {os.path.dirname(os.path.abspath(__file__))})")
        except Exception as e:
            print(f"[Orchestrator] ‚ùå An error occurred during the main scenario: {e}")
            import traceback
            traceback.print_exc()


async def main():
    # --- Select your OpenRouter Model Here ---
    # You can cycle through different models to test them
    # model_to_test = "mistralai/mixtral-8x7b-instruct" # Example paid/larger model
    # model_to_test = "google/gemini-flash-1.5" # Another good option
    model_to_test = "deepseek/deepseek-chat-v3-0324:free" # Or any other free model from earlier discussion
    # model_to_test = "anthropic/claude-3-haiku" # Fast and capable

    orchestrator = AIPentestOrchestrator(openrouter_model_name=model_to_test)
    await orchestrator.run_main_scenario()

if __name__ == "__main__":
    print("üöÄ Starting AI Pentest Orchestrator...")
    asyncio.run(main())